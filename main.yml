- name: Gather controller facts
  hosts: controller
  gather_facts: yes
  any_errors_fatal: true
  vars_files: [ "vars.yml" ]
  tasks:
      - name: Allow karaf port on controllers
        iptables:
            chain: INPUT
            action: insert
            destination_port: 8101
            protocol: tcp
            jump: ACCEPT
            ip_version: "{{ item }}"
        become: true
        with_items:
            - ipv4
            - ipv6

      - name: save iptables
        command: iptables-save
        become: true

      - name: discover IP karaf is listening on (IPv4 or v6)
        shell: |
            find /var/lib/config-data /opt -name "*karaf.shell.cfg*" 2>/dev/null | head -1 | xargs grep "sshHost =" | cut -d' ' -f3
        register: karaf_shell_ip
        become: true
        tags: ip


- name: Prepare and run Robot tests
  hosts: tester
  gather_facts: yes
  any_errors_fatal: true
  vars_files: [ "vars.yml" ]
  tasks:
      # TODO: remove once opendaylight jobs don't leave any residue (i.e.: routers) after running tempest tests
      - name: list overcloud resources - see whether there still are any leftovers
        shell: |
            . {{ ansible_user_dir }}/overcloudrc
            echo "openstack server list --all-projects"
            openstack server list --all-projects
            echo "openstack network list"
            openstack network list
            echo "openstack subnet list"
            openstack subnet list
            echo "openstack floating ip list"
            openstack floating ip list
            echo "openstack port list"
            openstack port list
            echo "openstack router list"
            openstack router list
            echo "openstack stack list"
            openstack stack list
            echo "openstack keypair list"
            openstack keypair list
            echo "openstack security group list"
            openstack security group list
        when: test.get('cleanup', {}).get('before', {})

      - name: install required RPMs
        package:
            name: "{{ item }}"
            state: present
        become: yes
        with_items:
            - python-virtualenv
            - gcc
        tags: overcloud-cleanup

      - name: setup shade_janitor to purge openstack resources possibly left after tempest run
        git:
            accept_hostkey: yes
            version: master
            repo: https://github.com/yazug/shade_janitor
            dest: shade_janitor
        tags: overcloud-cleanup
        when: test.get('cleanup', {}).get('before', {})

      - name: Create robot virtualenv with latest pip, setuptools
        pip:
            virtualenv: "/tmp/robot-venv"
            name: "{{ item }}"
            state: present
        with_items:
            - setuptools
            - pip
        tags: overcloud-cleanup
        when: test.get('cleanup', {}).get('before', {})

      - name: install shade_janitor
        shell: |
            set -xe
            . /tmp/robot-venv/bin/activate
            cd shade_janitor
                pip install -U pip
                pip install -r requirements.txt >> pip_install_requirements.txt
                pip list > pip_list.txt
        tags: overcloud-cleanup
        when: test.get('cleanup', {}).get('before', {})

      - name: run shade_janitor to cleanup overcloud
        shell: |
            set -x
            . {{ ansible_user_dir }}/overcloudrc

            openstack project list
            export TEMPEST_PROJECT_IDS="$(openstack project list --column ID --column Name | awk -F'|' '$0 ~ /tempest/ {print $2}')"
            for _id in $TEMPEST_PROJECT_IDS; do
               openstack project purge --project $_id
            done

            # the very hardcore way but for the moment neither shade_janitor, nor openstack project purge can't do clean this mess properly
            # will do it this way till we can use CSIT's built int Openstack cleanup
            for port_id in $(openstack port list -f value | awk '{print $1}') ; do openstack port delete $port_id; done
            for fip_id in $(openstack floating ip list -f value | awk '{print $1}') ; do openstack floating ip delete $port_id; done
            for subnet_id in $(openstack subnet list -f value | awk '{print $1}') ; do openstack subnet delete $subnet_id; done
            for net_id in $(openstack network list -f value | awk '{print $1}') ; do openstack network delete $net_id; done
            for router_id in $(openstack router list -f value | awk '{print $1}') ; do openstack router delete $router_id; done

            SHADE_JANITOR_OPTS="--cleanup"

            . /tmp/robot-venv/bin/activate
            cd shade_janitor
            python shade_janitor/janitor.py $SHADE_JANITOR_OPTS --floatingips >> overcloud_cleanup.log
            sleep 10
            python shade_janitor/janitor.py $SHADE_JANITOR_OPTS >> overcloud_cleanup.log
        register: overcloud_cleanup
        until: overcloud_cleanup.rc == 0
        retries: 5
        delay: 5
        ignore_errors: true
        tags: overcloud-cleanup
        when: test.get('cleanup', {}).get('before', {})

      - name: list overcloud resources
        shell: |
            . {{ ansible_user_dir }}/overcloudrc
            echo "openstack server list --all-projects"
            openstack server list --all-projects
            echo "openstack network list"
            openstack network list
            echo "openstack subnet list"
            openstack subnet list
            echo "openstack floating ip list"
            openstack floating ip list
            echo "openstack port list"
            openstack port list
            echo "openstack router list"
            openstack router list
            echo "openstack stack list"
            openstack stack list
            echo "openstack keypair list"
            openstack keypair list
            echo "openstack security group list"
            openstack security group list

      - name: set facts with ODL IPs
        set_fact:
            odl_system_ip: "[{{ hostvars['controller-0']['karaf_shell_ip']['stdout'] }}]"
            odl_system_1_ip: "{{ hostvars['controller-0']['karaf_shell_ip']['stdout'] }}"
            odl_system_2_ip: "{{ hostvars['controller-1']['karaf_shell_ip']['stdout'] }}"
            odl_system_3_ip: "{{ hostvars['controller-2']['karaf_shell_ip']['stdout'] }}"
        tags: ip

      - name: Enable internal_api on tester
        become: true
        shell: |
            ip link add link eth1 name eth1.20 type vlan id 20
            {% if '::' in odl_system_ip %}
            ip addr add {{ odl_system_ip | regex_replace('::\d+$', '::9999/64') }} dev eth1.20
            {% else %}
            ip addr add {{ odl_system_ip | regex_replace('\.\d+$', '.150/24') }} dev eth1.20
            {% endif %}
            ip link set eth1.20 up
        tags: ha

      - name: set workdir fact
        set_fact:
            workdir: "{{ ansible_user_dir }}/robot"
        tags: always

      - name: set logdir, dockerdir, resultsdir facts
        set_fact:
            logdir: "{{ workdir }}/logs"
            dockerdir: "{{ workdir }}/docker"
            resultsdir: "{{ workdir }}/results"
        tags: always

      - name: recreate workdir
        file:
            path: "{{ workdir }}"
            state: "{{ item }}"
        with_items:
            - absent
            - directory

      - name: create logdir, dockerdir, resultsdir
        file:
            path: "{{ item }}"
            state: directory
        with_items:
            - "{{ logdir }}"
            - "{{ dockerdir }}"
            - "{{ resultsdir }}"

      - name: install or update required software
        package:
            name: docker
            state: latest

      - name: pull container image
        docker_image:
            name: "{{ test.container.image.name }}"
            state: "{{ item }}"
            force: yes
        with_items:
            - absent
            - present
        tags: container-image

      - name: update odl_test repository and install odltools in docker container image
        shell: |
            echo "
            FROM {{ test.container.image.name }}
            RUN cd /home/opnfv/repos/odl_test/ && git pull origin master
            RUN pip install odltools
            " > {{ dockerdir }}/Dockerfile
            docker tag {{ test.container.image.name }} {{ test.container.image.name }}_before_git_pull
            docker build --network=host -t {{ test.container.image.name }} {{ dockerdir }}

      - name: set fact for overcloudrc
        set_fact:
            overcloudrc: "{{ workdir }}/overcloudrc"
        tags: always

      - name: remove old overcloudrc
        file:
          path: "{{ overcloudrc }}"
          state: absent

      - name: make a copy of overcloudrc as we will have to tweak it
        shell: |
          cp -f ~stack/overcloudrc {{ overcloudrc }}

      - name: find IPs of OpenDaylight and OpenStack nodes
        shell: |
            source ~stack/stackrc
            OPENSTACK_SERVERS="$(openstack server list)"
            set -a
            ODL_IP=$(egrep no_proxy {{ overcloudrc }} | grep -o '[^,]*$')
            OPENSTACK_CONTROL_NODE_1_IP=$(echo "$OPENSTACK_SERVERS" | egrep controller-0 | egrep -o '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+')
            OPENSTACK_CONTROL_NODE_2_IP=$(echo "$OPENSTACK_SERVERS" | egrep controller-1 | egrep -o '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+') || true
            OPENSTACK_CONTROL_NODE_3_IP=$(echo "$OPENSTACK_SERVERS" | egrep controller-2 | egrep -o '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+') || true
            OPENSTACK_COMPUTE_NODE_1_IP=$(echo "$OPENSTACK_SERVERS" | egrep compute-0 | egrep -o '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+')
            OPENSTACK_COMPUTE_NODE_2_IP=$(echo "$OPENSTACK_SERVERS" | egrep compute-1 | egrep -o '[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+') || true
            env | grep -E "ODL_IP|OPENSTACK_" >> {{ overcloudrc }}
        tags: ip

      - name: make sure cirros guest images are available for use by the tests
        shell: |
            source {{ overcloudrc }}
            if ! openstack image list | grep cirros-0.3.4-x86_64-uec; then
              curl -o /tmp/cirros3.4.img http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img
              glance image-create --name cirros-0.3.4-x86_64-uec --disk-format=qcow2 --container-format=bare --visibility=public --file=/tmp/cirros3.4.img
            fi
            if ! openstack image list | grep cirros-0.3.5-x86_64-disk; then
              curl -o /tmp/cirros3.5.img http://download.cirros-cloud.net/0.3.5/cirros-0.3.5-x86_64-disk.img
              glance image-create --name cirros-0.3.5-x86_64-disk --disk-format=qcow2 --container-format=bare --visibility=public --file=/tmp/cirros3.5.img
            fi
            if ! openstack flavor list | grep m1.nano; then
              openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 m1.nano
            fi

      - name: ODL Robot CSIT creates its own external network, so delete it an existing public network
        shell: |
            source {{ overcloudrc }}
            if openstack network show nova; then
                openstack network delete nova
            fi

      - name: robot CSIT is complaining about using v3 for VOLUME_API, but v2 seems to be ok - fixing overcloudrc
        replace:
            path: "{{ overcloudrc }}"
            regexp: "OS_VOLUME_API_VERSION=.*"
            replace: "OS_VOLUME_API_VERSION=2"

      - name: copy overcloudrc file to controller node and more tweaks
        shell: |
            source {{ overcloudrc }}

            scp -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                {{ overcloudrc }} heat-admin@$OPENSTACK_CONTROL_NODE_1_IP:/tmp/openrc

            # ODL Robot CSIT wants to use the ping command from the control node, so needs permissions
            ssh -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                heat-admin@$OPENSTACK_CONTROL_NODE_1_IP "sudo chmod u+s `which ping`;"

            # Simulate an external internet address on the control node
            ssh -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                heat-admin@$OPENSTACK_CONTROL_NODE_1_IP "sudo iptables -I INPUT -p tcp --dport 12345 -j ACCEPT; \
                                                         sudo iptables -I INPUT -p udp --dport 12345 -j ACCEPT; \
                                                         sudo ip tuntap add dev internet_tap mode tap; \
                                                         sudo ifconfig internet_tap up 10.9.9.9/24; \
                                                         sudo ip netns add pnf_ns; \
                                                         sudo ip link add pnf_veth0 type veth peer name pnf_veth1; \
                                                         sudo ip link set pnf_veth1 netns pnf_ns; \
                                                         sudo ip link set pnf_veth0 up; \
                                                         sudo ip netns exec pnf_ns ip link set dev lo up; \
                                                         sudo ip netns exec pnf_ns ip addr add dev pnf_veth1 10.10.10.253/24; \
                                                         sudo ip netns exec pnf_ns ip link set dev pnf_veth1 up; \
                                                         sudo ip netns exec pnf_ns ip route add default via 10.10.10.250; \
                                                         sudo ovs-vsctl add-port br-ex pnf_veth0; \
                                                         sudo ip addr add dev br-ex 10.10.10.250/24; \
                                                         sudo ip link set dev br-ex up; \
                                                         sudo ovs-vsctl --may-exist add-br $PUBLIC_BRIDGE -- set bridge $PUBLIC_BRIDGE other_config:disable-in-band=true other_config:hwaddr=f6:00:00:ff:01:01; \
                                                         sudo ovs-vsctl add-port br-ex compute1_vxlan -- set interface compute1_vxlan type=vxlan options:local_ip=$OPENSTACK_CONTROL_NODE_1_IP options:remote_ip=$OPENSTACK_COMPUTE_NODE_1_IP options:dst_port=9876 options:key=flow; \
                                                         sudo ovs-vsctl add-port br-ex compute2_vxlan -- set interface compute2_vxlan type=vxlan options:local_ip=$OPENSTACK_CONTROL_NODE_1_IP options:remote_ip=$OPENSTACK_COMPUTE_NODE_2_IP options:dst_port=9876 options:key=flow;"

            ssh -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                heat-admin@$OPENSTACK_COMPUTE_NODE_1_IP "sudo ovs-vsctl --may-exist add-br br-ex -- set bridge br-ex other_config:disable-in-band=true other_config:hwaddr=f6:00:00:ff:01:02; \
                                                        sudo ovs-vsctl add-port br-ex control_vxlan -- set interface control_vxlan type=vxlan options:local_ip=$OPENSTACK_COMPUTE_NODE_1_IP options:remote_ip=$OPENSTACK_CONTROL_NODE_1_IP options:dst_port=9876 options:key=flow;"


            ssh -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                    -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                    heat-admin@$OPENSTACK_COMPUTE_NODE_2_IP "sudo ovs-vsctl --may-exist add-br br-ex -- set bridge br-ex other_config:disable-in-band=true other_config:hwaddr=f6:00:00:ff:01:03; \
                                                            sudo ovs-vsctl add-port br-ex control_vxlan -- set interface control_vxlan type=vxlan options:local_ip=$OPENSTACK_COMPUTE_NODE_2_IP options:remote_ip=$OPENSTACK_CONTROL_NODE_1_IP options:dst_port=9876 options:key=flow;"


            # ODL Robot CSIT runs tempest tests from the control node, so installing that there
            ssh -i ~stack/.ssh/id_rsa -o StrictHostKeyChecking=no -o GlobalKnownHostsFile=/dev/null \
                -o UserKnownHostsFile=/dev/null -o LogLevel=error \
                heat-admin@$OPENSTACK_CONTROL_NODE_1_IP "sudo yum install -y openstack-tempest"

      - debug:
            var: hostvars[inventory_hostname]

      - name: run Robot
        shell: |
            set -x
            source {{ overcloudrc }}
            STACK_HOME=~stack
            robot_cmd="pybot \
                          --removekeywords wuks \
                          --xunit robotxunit.xml \
                          -c critical \
                          -e exclude \
                          -e skip_if_oxygen \
                          -d {{ resultsdir }} \
                          -v BUNDLEFOLDER:/opt/opendaylight \
                          -v CONTROLLER_USER:heat-admin \
                          -v DEFAULT_LINUX_PROMPT:\$ \
                          -v DEFAULT_LINUX_PROMPT_STRICT:]\$ \
                          -v DEFAULT_USER:heat-admin \
                          -v DEVSTACK_DEPLOY_PATH:/tmp \
                          -v HA_PROXY_IP:$ODL_IP \
                          -v HA_PROXY_1_IP:$ODL_IP \
                          -v HA_PROXY_2_IP:$ODL_IP \
                          -v HA_PROXY_3_IP:$ODL_IP \
                          -v NUM_ODL_SYSTEM:{{ groups['controller'] | length }} \
                          -v NUM_OS_SYSTEM:3 \
                          -v NUM_TOOLS_SYSTEM:0 \
                          -v OPENSTACK_TOPO:0cmb-1ctl-2cmp \
                          -v ODL_SNAT_MODE:conntrack \
                          -v ODL_STREAM:{{ osp_version[test.osp.version | int].opendaylight_release }} \
                          -v OS_CONTROL_NODE_IP:$ODL_IP \
                          -v OS_CONTROL_NODE_1_IP:$OPENSTACK_CONTROL_NODE_1_IP \
                          -v OS_CONTROL_NODE_2_IP:$OPENSTACK_CONTROL_NODE_2_IP \
                          -v OS_CONTROL_NODE_3_IP:$OPENSTACK_CONTROL_NODE_3_IP \
                          -v OPENSTACK_BRANCH:stable/{{ osp_version[test.osp.version | int].openstack_branch }} \
                          -v OS_COMPUTE_1_IP:$OPENSTACK_COMPUTE_NODE_1_IP \
                          -v OS_COMPUTE_2_IP:$OPENSTACK_COMPUTE_NODE_2_IP \
                          -v ODL_SYSTEM_IP:{{ odl_system_ip }} \
                          -v ODL_SYSTEM_1_IP:{{ odl_system_1_ip }} \
                          {%- if hostvars['controller-1'] is defined %}
                          -v ODL_SYSTEM_2_IP:{{ odl_system_2_ip }} \
                          {% endif %}
                          {%- if hostvars['controller-2'] is defined %}
                          -v ODL_SYSTEM_3_IP:{{ odl_system_3_ip  }} \
                          {% endif %}
                          -v OS_USER:heat-admin \
                          -v ODL_ENABLE_L3_FWD:yes \
                          -v ODL_SYSTEM_USER:heat-admin \
                          -v ODL_SYSTEM_PROMPT:\$ \
                          -v PRE_CLEAN_OPENSTACK_ALL:True \
                          -v PUBLIC_PHYSICAL_NETWORK:datacentre \
                          -v RESTCONFPORT:8081 \
                          -v ODL_RESTCONF_USER:{{ test.opendaylight.username }} \
                          -v ODL_RESTCONF_PASSWORD:{{ test.opendaylight.password }} \
                          -v KARAF_PROMPT_LOGIN:'{{ test.karaf.prompt.login }}' \
                          -v KARAF_PROMPT:'{{ test.karaf.prompt.line }}' \
                          -v SECURITY_GROUP_MODE:stateful \
                          -v USER_HOME:/home/stack \
                          -v WORKSPACE:{{ workdir }}/workspace  \
                          -v USER:heat-admin \
                          -v TOOLS_SYSTEM_IP:localhost \
                          -v NODE_KARAF_COUNT_COMMAND:\"sudo docker exec opendaylight_api /bin/bash -c 'ps axf | grep org.apache.karaf | grep -v grep | wc -l' || echo 0\" \
                          -v NODE_START_COMMAND:\"sudo docker start opendaylight_api\" \
                          -v NODE_KILL_COMMAND:\"sudo docker stop opendaylight_api\" \
                          -v NODE_STOP_COMMAND:\"sudo docker stop opendaylight_api\" \
                          -v NODE_FREEZE_COMMAND:\"sudo docker stop opendaylight_api\" \
                          -v NODE_ROLE_INDEX_START:0 \
                          -v of_port:6653 "

            docker run -i --net=host -v {{ workdir }}:{{ workdir }}:Z \
                                     -v /home/stack/.ssh:/home/stack/.ssh:Z \
                                     -v $STACK_HOME:$STACK_HOME \
                                     -v {{ overcloudrc }}:{{ overcloudrc }} \
                                     {{ test.container.image.name }} \
                          /bin/bash -c "source {{ overcloudrc }}; \
                                        $robot_cmd {{ test.tests.replace(',', ' ') }};" \
                          2>&1 | tee -a {{ logdir }}/docker_run_$TEST_NAME.log
        tags: run-tests

      - name: Ensure results target directory exists
        file:
            dest: "{{ inventory_dir }}/../../../robot/"
            state: directory
        delegate_to: localhost
        tags: fetch-results

      - name: find results files
        find:
            paths: "{{ resultsdir }}"
            patterns: '*.xml,*.html'
            recurse: yes
        register: robot_results_files
        tags: fetch-results

      - name: Fetch results files
        fetch:
            src: "{{ item.path }}"
            dest: "{{ inventory_dir }}/../../../robot/"
            flat: yes
            fail_on_missing: no
        with_items: "{{ robot_results_files.files }}"
        tags: fetch-results

      - name: Fetch JUnit XML results file
        fetch:
          src: "{{ item.path }}"
          dest: "{{ inventory_dir }}/robot_results/"
          flat: yes
          fail_on_missing: no
        with_items: "{{ robot_results_files.files }}"
        tags: fetch-results
